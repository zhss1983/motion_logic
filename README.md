# Проект парсинга документации сайтов Burger King, KFC, McDonalds (Вкусно и точка).

## Стек технологий:

**[logging](https://docs.python.org/3/library/logging.html),
 [requests_cache](https://requests-cache.readthedocs.io/en/stable/),
 [pprint](https://docs.python.org/3/library/pprint.html),
 [Django Rest Framework](https://www.django-rest-framework.org/),
 [Docker-Compose](https://docs.docker.com/compose/gettingstarted/),
 [pre-commit](https://pre-commit.com/index.html),
 [pidantic](https://pydantic-docs.helpmanual.io/),
 [black](https://pypi.org/project/black/)**

## Описание:

Данное тестовое задание было выполнено в течении 3 рабочих дней по заказу компании [ООО Моушен Лоджик](https://motionlogic.ru/).
Цель: написать Django проект содержащий БД, описать модель Restaurant и проработать парсинг всех ресторанов из открытых источников для компании Burger King, KFC и McDonalds. Посчитать сколько всего ресторанов у каждого из предприятий и передать их в pandas для дальнейшего анализа.

Дополнительно прозвучало задание проанализировать конкурентную среду в Москве, подготовить и представить выводы в свободной форме.

Каких либо ограничений не накладывали.

## Решение:

Исходя из собеседования был сделан вывод что предпочтительными будут DRF и БД PostgreSQL. Так как мне не известно в каких условиях проект будет запущен я изначально рассчитывал на запуск всего програмного комплекса в контейнере. Для этого была развёрнута минимальная конфигурация Django с настройками под PostgreSQL на базе Docker-Compose. Попутно были задействованы библиотеки помогающие разработчику писать код чище: pre-commit и black.

Далее я составил модель для сохранения ресторанов (самое основное, название, адрес, геокоординаты, телефон, владелец, описание). На этом этапе я позволил себе изменить название модели с Restaurant на Organisations так как это с моей точки зрения наиболее адекватное название. Однако если вдруг потребовалось бы именно Restaurant - поменять обратно совершенно не проблема (даже в БД с уже имеющимися данными).

Далее работа с DRF совершенно стандартная - добавить эндпоинты на добавление в БД и извлечение из неё нужных данных и всё.

По парсеру пришлось немного поразбираться - сбило с толку необходимость выбирать данные с карт, однако после того как были вычленены запросы к серверу и ответы с данными - всё пошло гладко. В Django был добавлен новый модуль и новые эндпоинты. При обращении к ним происходит запрос к API сайтов по выявленным адресам и получение json с требуемыми данными. Добавлен класс для парсинга и внесения их в БД, произведён париснг и полученные данные выгружены в файл.
У меня не написан програмный код для запроса моего же сервера для выгрузки с него данных - я посчитал это на текущий момент излишним. Можно было и это автоматизировать, но такой задачи не стояло. Данные были конвертированы с помощью библиотеки pandas в CSV формат и записаны на диск для того что бы с ними можно было работать где либо ещё помимо моих скриптов. Так же показал стандартную загрузку CSV файла в pandas и фильтрацию значений по колонке "владельца" ("owner").

Что касательно анализа конкурентной среды в Москве и подготовки выводов в свободной форме: экономичесикого образования не имею, знаниями в данной области не обладаю.
Не считаю возможным делать какие либо выводы без соответствующих компетенций. Как простой обыватель могу сказать по простому, в центре москвы от кафешек рябит в глазах, они там по всюду. По окрайнам уже полегче.

Считаю задачу выполненной, да ещё есть огрехи связанные с чистотой написанного кода, может где-то не совсем корректно написаны классы с точки зрения SOLID, но я работал в режиме ограниченного времени так как сам назначил дату сдачи и не хотел пропустить соответствующий дэдлайн. По сути этот проект написан исходя из того что он должен работать и выполнять все возложенные на него функции. Далее его можно будет улучшать при желании.

## Запуск

Если вы собираетесь работать из командной строки в **windows**, вам может
 потребоваться Bash. скачать его можно по ссылке:
 [GitBash](https://gitforwindows.org/) ([Git-2.33.0.2-64-bit.exe](https://github.com/git-for-windows/git/releases/download/v2.33.0.windows.2/Git-2.33.0.2-64-bit.exe)).

Так же при работе в **windows** необходимо использовать **python** вместо
 **python3**

Последнюю версию **python** ищите на официальном сайте
 [https://www.python.org/](https://www.python.org/downloads/)

Клонировать репозиторий и перейти в него в командной строке:

```/bin/bash
git clone https://github.com/zhss1983/motion_logic
```

```/bin/bash
cd motion_logic
```

Создать и активировать виртуальное окружение:

```/bin/bash
python -m venv env
```

- linux
```/bin/bash
source env/bin/activate
```
- windows
```/bin/bash
source env/Scripts/activate
```

Установить зависимости из файла **restaurant/requirements.txt**:

```/bin/bash
python -m pip install --upgrade pip
```

```/bin/bash
pip install -r requirements.txt
```

Запуск Django:

Для того что бы проект минимально заработал в ./restaurant/restaurant/setup.py требуется константу DEBUG изменить на True. Таким образом можно будет с минимальными усилиями запустить проект из терминала не прибегая к установке Docker или PostgerSQL на компьютер.

```/bin/bash
cd ./restaurant
pip install -r requirement.txt
python manage.py migrate
python manage.py createsuperuser
python manage.py runserver
```

Для запуска с БД PostgreSQL потребуется запуск контейнера. Для этого необходимо предварительно [установить Docker](https://docs.docker.com/engine/install/).

Проект можно запустить из папки restourant с помощью команды:

```/bin/bash
docker-compose build
docker-compose up
```

Сервер запустится по адресу http://127.0.0.1:8000

## Парсинг официальных сайтов Burger King, KFC и mcDonalds.

Для старта парсинга необходимо запустить POST запрос по адресам (полный текст ответа можно посмотреть в папке restouran/parser/result/):

[http://127.0.0.1:8000/api/v1/parser/burger_king](http://127.0.0.1:8000/api/v1/parser/burger_king)

[http://127.0.0.1:8000/api/v1/parser/kfc](http://127.0.0.1:8000/api/v1/parser/kfc)

[http://127.0.0.1:8000/api/v1/parser/mcdonalds](http://127.0.0.1:8000/api/v1/parser/mcdonalds)

В результате будут произведены запросы к соответствующим api компаний и полученные данные будут добавлины в БД, если их там ранее не было.

Ответом будет количество добавленных записей.

Можно получить доступ к соответствующим таблицам по адресам:

[http://127.0.0.1:8000/api/v1/organisation/](http://127.0.0.1:8000/api/v1/organisation)
 Возможен поиск (?search=<название фирмы или адрес>).

[http://127.0.0.1:8000/api/v1/owner/](http://127.0.0.1:8000/api/v1/owner)
 Возможен поиск (?search=<название фирмы владельца>).

[http://127.0.0.1:8000/api/v1/phone/](http://127.0.0.1:8000/api/v1/phone)

[http://127.0.0.1:8000/api/v1/object_type/](http://127.0.0.1:8000/api/v1/object_type)

Отдельно добавил адрес [http://127.0.0.1:8000/api/v1/organisation_by_owner/](http://127.0.0.1:8000/api/v1/organisation_by_owner)
для выгрузки списка организаций в формате json (ручками). Так был выгружен файл pandas/test.json.

## Посчитать сколько всего ресторанов у каждого из предприятий.
Количество было подсчитано через запрос SQL: SQL.txt

Так же количесиво подсчитано через Pandas. Скрипт лежит в pandas/pandas_converter.py Данный скрипт конвертирует json в csv и считает количество ресторанов с уникальными владельцами (KFC, Burger King и McDonalds).
